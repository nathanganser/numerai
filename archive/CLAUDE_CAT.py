# -*- coding: utf-8 -*-
"""Playing with GPUs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gTPAnYM9Br05UXINQEQjC7pXxtmaHfBZ
"""


# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from numerapi import NumerAPI
import json
import lightgbm as lgb
from numerai_tools.scoring import numerai_corr, correlation_contribution

# Initialize NumerAPI
napi = NumerAPI()

# Set data version
DATA_VERSION = "v5.0"
PERCENTAGE_DATA_USED = 100
ITERATIONS = 300000 # 200000
LEARNING_RATE = 0.008
DEPTH = 8 # 8
FEATURESET = 'all' # medium, all, small

# Download and load feature metadata
napi.download_dataset(f"{DATA_VERSION}/features.json")
feature_metadata = json.load(open(f"{DATA_VERSION}/features.json"))
feature_set = feature_metadata["feature_sets"][FEATURESET]

# Download and load training data
napi.download_dataset(f"{DATA_VERSION}/train.parquet")
train = pd.read_parquet(
    f"{DATA_VERSION}/train.parquet",
    columns=["era", "target"] + feature_set
)

# Reduce train to only be 1% of the data (to speed up training)
train = train.sample(frac=PERCENTAGE_DATA_USED/100, random_state=42)

import pandas as pd
from sklearn.model_selection import train_test_split

# Assuming 'train' is your original DataFrame
train_df, val_df = train_test_split(train, test_size=0.2, random_state=42)

# Reset indices for both DataFrames
train_df = train_df.reset_index(drop=True)
val_df = val_df.reset_index(drop=True)

# Print the shapes of the resulting DataFrames
print(f"Training set shape: {train_df.shape}")
print(f"Validation set shape: {val_df.shape}")

from catboost import CatBoostClassifier, Pool, metrics, cv

# prompt: Generate X_train, y_train, X_test, y_test

# Define features and target
features = feature_set
target = "target"

# Create training data
X_train = train_df[features]
y_train = train_df[target]

# Create validation data
X_test = val_df[features]
y_test = val_df[target]

import numpy
from catboost import CatBoostRegressor, Pool, metrics, cv

# Create CatBoost pools
train_pool = Pool(X_train,
                  y_train,
)

test_pool = Pool(
    X_test,
    y_test,
)

model = CatBoostRegressor(
    iterations=ITERATIONS,
    learning_rate=LEARNING_RATE,
    depth=DEPTH,
    loss_function='RMSE',
    od_type='Iter',
    od_wait = 500,
    bagging_temperature = 0.3,
    random_strength=0.3,
    leaf_estimation_iterations=10,
    leaf_estimation_method='Newton',
    task_type='GPU',
    thread_count=-1)

model.fit(train_pool, eval_set=test_pool, verbose=500, use_best_model=True);


import pandas as pd
import cloudpickle
import logging
import requests

# Download dataset
napi.download_dataset(f"{DATA_VERSION}/live.parquet")

def predict(live_features: pd.DataFrame) -> pd.DataFrame:
    live_predictions = model.predict(live_features[feature_set])
    submission = pd.Series(live_predictions, index=live_features.index)
    return submission.to_frame("prediction")

# Serialize and upload prediction function
logging.info("Serializing and uploading prediction function...")
p = cloudpickle.dumps(predict)
with open("model.pkl", "wb") as f:
    f.write(p)

# Test prediction function using the pickle file
logging.info("Testing prediction function from pickle file...")
live_data = pd.read_parquet(f"{DATA_VERSION}/live.parquet", columns=["era"] + feature_set)

# Load the pickled prediction function
with open("model.pkl", "rb") as f:
    loaded_predict = cloudpickle.loads(f.read())

# Use the loaded function to make predictions
predictions = loaded_predict(live_data)
print(f"Predictions shape: {predictions.shape}")
print(f"Predictions head:\n{predictions.head()}")

url = "https://api.bytescale.com/v2/accounts/12a1yew/uploads/form_data"
headers = {"Authorization": "Bearer public_12a1yewAHfRPdqAXnHXQDib1RwoJ"}
files = {"file": open(f"model.pkl", "rb")}
response = requests.post(url, headers=headers, files=files)
if response.status_code == 200:
  print("File uploaded successfully.")
else:
  print(f"File upload failed with status code: {response.status_code}")
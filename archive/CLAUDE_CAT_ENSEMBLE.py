# -*- coding: utf-8 -*-
"""Playing with GPUs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gTPAnYM9Br05UXINQEQjC7pXxtmaHfBZ
"""



# Import necessary libraries
import numpy as np
import pandas as pd
import requests
import matplotlib.pyplot as plt
from numerapi import NumerAPI
import json
import pandas as pd
from sklearn.model_selection import train_test_split
from catboost import CatBoostRegressor, Pool, metrics, cv


# Initialize NumerAPI
napi = NumerAPI()

# Set data version
DATA_VERSION = "v5.0"
PERCENTAGE_DATA_USED = 100
ITERATIONS = 100000
DEPTH = 6
LEARNING_RATE = 0.009
TARGET = "target"
SECONDARY_TARGETS_COUNT = 4
TEST_SPLIT = 0.2
FEATURESET = 'all' # medium, all, small

# Download and load feature metadata
napi.download_dataset(f"{DATA_VERSION}/features.json")
feature_metadata = json.load(open(f"{DATA_VERSION}/features.json"))
feature_set = feature_metadata["feature_sets"][FEATURESET]

# Download and load training data
napi.download_dataset(f"{DATA_VERSION}/validation.parquet")
train = pd.read_parquet(
    f"{DATA_VERSION}/validation.parquet"
)


train = train.sample(frac=PERCENTAGE_DATA_USED/100, random_state=42)


# Define features and target
features = feature_set
models = []
count = 0
# Assuming 'train' is your DataFrame
# Select all columns that start with 'target_'
all_targets = [col for col in train.columns if col.startswith('target_')]

# Calculate the number of NaN values for each target
nan_counts = train[all_targets].isna().sum().sort_values()

# Select the 3 targets with the least NaN values
selected_targets = nan_counts.index[:SECONDARY_TARGETS_COUNT].tolist()

if 'era' in train.columns:
    for col in selected_targets:
        train[col] = train.groupby('era')[col].transform(lambda x: x.fillna(x.median()))

# Identify secondary targets
secondary_targets = [col for col in selected_targets
                     if col != TARGET
                     and not train[col].isna().any()]
print(f"Number of secondary targets: {len(secondary_targets)}")

targets = [TARGET] + secondary_targets

training_data, meta_data = train_test_split(train, test_size=TEST_SPLIT, random_state=42)

for target in targets:
  print(f"Training on target: ", target)
  # Create training data
  train_df, val_df = train_test_split(training_data, test_size=TEST_SPLIT, random_state=count)
  X_train = train_df[features]
  print("Data size", X_train.shape)
  y_train = train_df[target]

  # Create validation data
  X_test = val_df[features]
  y_test = val_df[target]

  # Create CatBoost pools
  train_pool = Pool(X_train,
                    y_train,
  )

  test_pool = Pool(
      X_test,
      y_test,
  )

  model = CatBoostRegressor(
      iterations=ITERATIONS,
      learning_rate=LEARNING_RATE,
      depth=DEPTH,
      loss_function='RMSE',
      od_type='Iter',
      od_wait = 500,
      task_type='GPU')

  model.fit(train_pool, eval_set=test_pool, verbose=round(ITERATIONS/3));
  models.append(model)
  count +=1

meta_train_data, meta_test_data = train_test_split(meta_data, test_size=TEST_SPLIT, random_state=count)

# Generate meta-features (predictions from base models)
meta_features_train = np.column_stack([
    model.predict(meta_train_data[features]) for model in models
])
meta_features_test = np.column_stack([
    model.predict(meta_test_data[features]) for model in models
])

# Use a single target for meta-model training
y_train = meta_train_data['target']
y_test = meta_test_data['target']

# Create CatBoost pools
train_pool = Pool(meta_features_train, y_train)
test_pool = Pool(meta_features_test, y_test)

# Train the meta-model
print("Training meta-model")
meta_model = CatBoostRegressor(
    iterations=ITERATIONS,
    learning_rate=LEARNING_RATE,
    depth=DEPTH,
    loss_function='RMSE',
    od_type='Iter',
    od_wait=500,
    task_type='GPU'
)

meta_model.fit(train_pool, eval_set=test_pool, verbose=round(ITERATIONS/3))

# Evaluate the meta-model
predictions = meta_model.predict(meta_features_test)

"""# Prediction & pickle"""

import pandas as pd
import cloudpickle
import logging

# Download dataset
napi.download_dataset(f"{DATA_VERSION}/live.parquet")

def predict(live_features: pd.DataFrame) -> pd.DataFrame:
    base_features = live_features[features]

    # Generate predictions from all base models
    base_predictions = np.column_stack([
        model.predict(base_features) for model in models
    ])

    # Use the meta-model to make final predictions
    final_predictions = meta_model.predict(base_predictions)

    submission = pd.Series(final_predictions, index=live_features.index)
    return submission.to_frame("prediction")

# Serialize and upload prediction function
logging.info("Serializing and uploading prediction function...")
p = cloudpickle.dumps(predict)
with open("model.pkl", "wb") as f:
    f.write(p)

# Test prediction function using the pickle file
logging.info("Testing prediction function from pickle file...")
live_data = pd.read_parquet(f"{DATA_VERSION}/live.parquet", columns=["era"] + features)

# Load the pickled prediction function
with open("model.pkl", "rb") as f:
    loaded_predict = cloudpickle.loads(f.read())

# Use the loaded function to make predictions
predictions = loaded_predict(live_data)
print(f"Predictions shape: {predictions.shape}")
print(f"Predictions head:\n{predictions.head()}")

feature_importances = meta_model.get_feature_importance(train_pool)
feature_names = X_train.columns
for score, name in sorted(zip(feature_importances, feature_names), reverse=True):
    print('{}: {}'.format(name, score)) if score > 0 else None